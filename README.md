# GestoLingo
<img src="https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/images/logo_tfm.jpeg" width=500px>

# Autores
- [Guillermo Rojo Santos](https://github.com/GuillermoRojoSantos)
- [Jos√© Antonio D√≠az](https://github.com/jada1998velez)
- [Gabriel Postigo](https://github.com/GabrielPostigo)
---
# Descripci√≥n y Justificaci√≥n

üëã ¬°Hola!,¬øTe gustat√≠a aprender m√°s sobre la Lengua de Signos Espa√±ola? üëã

Entonces debes de conocer **GestoLingo**, la mejor APP para aprender esta lengua. ü§ô

Pero, ¬øqu√© es GestoLingo? ü§î

**GestoLingo** es una herramienta de interpretaci√≥n y traducci√≥n en tiempo real del **LSE** (Lengua de Signos Espa√±ola) que usa `CV` (Computer Vision) y `NLP` (Natural Language Processing) para identificar, interpretar y transformar palabras simples del LSE en palabras equivalentes en espa√±ol.

El principal prop√≥sito de esta herramienta es ayudar a gente con impedimentos del habla a aprender a comunicarse mediante el uso del Lenguaje de Signos en Espa√±ol. ‚òù

¬øC√≥mo ha sido logrado? üò±

Gracial al grupo de estudiantes del **M√°ster de Inteligencia Artificial y Big Data**, logrando conseguir crear un modelo  con Inteligencia Artificial capaz de realizar esta dif√≠cil tarea ü¶æü§ñ

# √çndice


[Arquitectura del proyecto](#arquitectura-del-proyecto-1)
--
[Tecnolog√≠as utilizadas](#tecnologc3adas-utilizadas-1)
--
[Obtenci√≥n de datos](#obtencic3b3n-de-datos-1)
--
[Limpieza de datos](#limpieza-de-datos-1)
--
[Exploraci√≥n y visualizaci√≥n de los datos](#preparacic3b3n-de-los-datos-1)
--
[Preparaci√≥n de los datos](#preparacic3b3n-de-los-datos-1)
--
[Entrenamiento del modelo y comprobaci√≥n del rendimiento](#entrenamiento-del-modelo-y-comprobacic3b3n-del-rendimiento-1)
--
[Procesamiento de Lenguaje Natural](#procesamiento-de-lenguaje-natural-1)
--
[Aplicaci√≥n Web](#aplicacic3b3n-web-1)
--
[Bibliograf√≠a](#bibliografc3ada-1)
--
[Conclusi√≥n](#conclusic3b3n-1)
--

# Arquitectura del proyecto

## Diagrama
Este proyecto recoge 2 campos fundamentales de la Inteligencia Artificial, como lo son el NLP y el Reconocimiento de Im√°genes. Se ha decidido dividir el proyecto en 3 campos fundamentales: 

* **Data:** Para la recogida y procesamiento de datos
* **Modelo:** Para la realizaci√≥n de una red neuronal que ser√° el cerebro de la aplicaci√≥n
* **Web:** Para mostrar de forma m√°s interactiva el resultado final.

<img src = 'https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/images/ArquitecturaGestoLingo.png' width = 800px>

A continuaci√≥n se explicar√° de manera m√°s detallada cada apartado:

**Data:**
* Se recoger√°n los datos para el entrenamiento con `grabaciones` realizadas por los propios participantes del proyecto para posteriormente tratarlas y exportarlas a un DataFrame.
* Se realizar√°n t√©cnicas de `Web Scraping` a la p√°gina web [DILSE](https://fundacioncnse-dilse.org/) para la obtenci√≥n de Videos y comprobaci√≥n de palabras. Esos datos ser√°n tratados y cargados a un Bucket de S3 el cual servir√° como ‚ÄúBase de Datos‚Äù.

**Modelo:**


* En primer lugar, gracias a la librer√≠a `Keras`, se crear√° una red neuronal, la cual obtendr√° datos de entrenamiento y validaci√≥n. Posteriormente con la librer√≠a `TensorFlow` entrenaremos dicho modelo.


**Web:**

Se crear√° una aplicaci√≥n web con Streamlit, la cual permanecer√° en local debido a las limitaciones obtenidas por la plataforma a la hora de albergarla. La web estar√° dividida en las siguientes pesta√±as:

* **√çndice:** Donde tendremos informaci√≥n principal sobre la p√°gina
* **Aprende:** En esta pesta√±a podremos aprender varias de las palabras del lenguaje de signos, gracias a que podemos buscar en el bucket de `S3` los v√≠deos para la demostraci√≥n de √©stas.
* **Practicar:** Aqu√≠ estar√° albergado el modelo, podremos activar la opci√≥n de ‚ÄúC√°mara‚Äù para en tiempo real realizar algunas de las palabras que anteriormente hemos podido aprender.
* **Configuraci√≥n:** Esta pesta√±a es necesaria para poder acceder al bucket de `AWS`, escribiendo las credenciales, diferenciando entre cuentas profesionales y corporativas

## Jerarqu√≠a de directorios

[Data](https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/data): Directorio donde se almacenan los datos que se ir√°n generando para el entrenamiento del modelo.
* [DataFrames](https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/data/dataFrames):  Directorio donde se almacenan los archivos HDF5 antes de la limpieza, generados al pasar las fotos por ‚Äòsrc/new_process_hands.py‚Äô.

* [model](https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/data/model): Directorio donde se guarda el modelo tras ser creado por ‚Äòsrc/train_model.py‚Äô .
    * [GestoLingo.keras](https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/data/model/GestoLingo.keras): Modelo entrenado.
    * [hand_landmarker.tas](https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/data/model/hand_landmarker.tas): Archivo que se usar√° para realizar las inferencias en ‚Äòsrc/new_extract_hands.py‚Äô.

* [treatedDF](https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/data/treatedDF): Directorio donde se almacenan los DataFrames tras ser limpiados en el cuaderno ‚Äònotebooks/Limpieza_y_Exploraci√≥n_de_datos.ipynb‚Äô.

[words](https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/words): Directorio donde se almacenan las palabras, con sus samples y frames de cada gesto obtenido por src/new_extract_hands.py.

[images](https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/words): Directorio donde est√°n las imagenes que usaremos en README.md.

[notebooks](https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/notebooks): Directorio se encuentran los cuadernos Jupyter.

* [Limpieza_y_Exploraci√≥n_de_datos.ipynb](https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/notebooks/Limpieza_y_Exploraci√≥n_de_datos.ipynb): Cuaderno Jupyter que limpia los Dataframes almacenados en ‚Äòdata/dataframes‚Äô y los almacena en ‚Äòdata/treatedDF‚Äô

* [WebScrapingGestolingo.ipynb](https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/notebooks/WebScrapingGestolingo.ipynb):  Cuaderno Jupyter donde se realiza un scrapeo al diccionario Dilse, descarga los videos y los sube a un bucket en S3.

[src](https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/src): Directorio donde se almacenan los scripts principales.

* [legacy](https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/src/legacy): Directorio donde quedan guardados los scripts de una versi√≥n antigua, en la que se us√≥ un modelo de mediapipe diferente.

    * [extract_hands.py](https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/src/legacy/extract_hands.py): Script de extracci√≥n de gestos y almacenado en local.

    * [process_hands.py](https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/src/legacy/process_hands.py): Script para procesar y obtener los archivos HDF5 de las palabras almacenadas en el directorio words.


* [streamlit](https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/src/streamlit): Directorio donde se almacenan los scripts y archivos relacionados con la aplicaci√≥n web.

    * [images](https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/src/streamlit/images): Directorio donde se almacenan las im√°genes usadas en la web.

    * [streamlit_main.py](https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/src/streamlit/streamlit_main.py): Script principal de la web.

    * [style.css](https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/src/streamlit/style.css): Archivo de la hoja de estilos CSS para la web.

* [new_extract_hands.py](https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/src/new_extract_hands.py): Script para capturar gestos y almacenar los frames en local, en la carpeta words.

* [new_process_hands.py](https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/src/new_process_hands.py): Script que procesa los gestos almacenados en words y genera un archivo HDF5 correspondiente en la carpeta data/dataframes.

* [train_model.py](https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/src/train_model.py): Script que sirve para crear, entrenar y guardar el modelo entrenado en la carpeta data/models.

# Tecnolog√≠as utilizadas

<img src="images/aws.png" width="100px" style="margin: 2px;">
<img src="images/colab.png" width="100px" style="margin: 2px;">
<img src="images/css.png" width="100px" style="margin: 2px;">
<img src="images/git.png" width="100px" style="margin: 2px;">
<img src="images/github.png" width="100px" style="margin: 2px;">
<img src="images/html.png" width="100px" style="margin: 2px;">
<img src="images/jupyter.png" width="100px" style="margin: 2px;">
<img src="images/keras.png" width="100px" style="margin: 2px;">
<img src="images/mediapipe.png" width="100px" style="margin: 2px;">
<img src="images/miniconda.png" width="100px" style="margin: 2px;">
<img src="images/opencv.png" width="100px" style="margin: 2px;">
<img src="images/pycharm.png" width="100px" style="margin: 2px;">
<img src="images/python.png" width="100px" style="margin: 2px;">
<img src="images/streamlit.png" width="100px" style="margin: 2px;">
<img src="images/tensorflow.png" width="100px" style="margin: 2px;">
<img src="images/visual.png" width="100px" style="margin: 2px;">


* **Open CV:** Biblioteca de visi√≥n por computadora.
* **Python:** Lenguaje de programaci√≥n de alto nivel.
* **Streamlit:** Marco de creaci√≥n de aplicaciones web interactivas con Python.
* **Mediapipe:** Biblioteca para an√°lisis de datos de medios (im√°genes y videos).
* **AWS:** Amazon Web Services, plataforma de servicios en la nube.
* **Google Colab:** Entorno de cuadernos colaborativos basado en la nube.
* **Jupyter:** Entorno interactivo para escribir y ejecutar c√≥digo en cuadernos.
* **Miniconda:** Distribuci√≥n de Conda m√°s ligera para gesti√≥n de paquetes.
* **Tensorflow:** Biblioteca de c√≥digo abierto para aprendizaje autom√°tico.
* **Keras:** Interfaz de alto nivel para redes neuronales, integrada en TensorFlow.
* **html:** Lenguaje de marcado para la creaci√≥n de p√°ginas web.
* **CSS:** Lenguaje para el dise√±o y estilo de p√°ginas web.
* **Cx-Freeze:** Herramienta para crear ejecutables a partir de programas de Python.
* **Lupas Rename:** Aplicaci√≥n para renombrar muchos archivos.
* **Github:** Plataforma de desarrollo colaborativo utilizando Git.
* **Git:** Sistema de control de versiones distribuido.
* **Visual Studio Code:** Editor de c√≥digo fuente.
* **Pycharm:** Entorno de desarrollo integrado (IDE) para Python.



# Obtenci√≥n de datos

### 1¬∫ intento
En un principio se probb√≥ a obtener los datos de la fuente  "[**Diccionario de la Lengua de Signos Espa√±ola**](https://fundacioncnse-dilse.org/)", √©sta p√°gina nos ofrece un buscador en el que nosotros podremos elegir la palabra que queramos aprender y nos aparecer√° un video descargable para esa palabra.

En primer lugar utilizamos t√©cnicas de Web Scraping desde Google Colab para la obtenci√≥n de esos v√≠deos.
Utilizaremos la librer√≠a `BeautifulSoup` y la librer√≠a `request` para la obtenci√≥n de dichos datos

Cuando obtengamos los v√≠deos de la web, es necesario dividirlo en frames, por lo que usaremos la siguiente funci√≥n para dividir el v√≠deo y guardar esos frames en carpetas

(***NOTA:** Vamos a usar una sola palabra como ejemplo, pero se deber√° de hacer despu√©s con todas las que queramos*)

<img src = 'https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/images/scraping.png' width = 800px>

Para guardar las im√°genes y no tener que repetir el proceso cada vez que iniciamos el colab, utilizamos el siguiente c√≥digo

<img src = 'https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/images/descargar_directorio.png' width = 800px>

Usando el modelo de Mediapipe de detecci√≥n de manos, hacemos una predicci√≥n en cada frame de la posici√≥n de las manos, para ello utilizamos el siguiente c√≥digo:

<img src = 'https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/images/marcar_mediapipe.png' width = 800px>

Con el modelo de Mediapipe que se ha mencionado antes, se ha creado una clase con funcionalidades dedicadas al seguimiento de manos (el archivo `src/seguimiento_manos.py`), en el cu√°l se establecen las funciones para ubicar una sola mano o ambas, detectar la posici√≥n de de estas, cuantos dedos hay levantados y la distancia que hay entre los dedos de cada mano.

Vamos a mostrar un ejemplo de como se ver√≠a un frame de la palabra `hola` con la detecci√≥n del mediapipe.

<img src = 'https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/images/hola_15_marked.png' width = 800px>

### 2¬∫ intento y con el que nos quedamos
Despu√©s de probar este m√©todo y ver que no sabiamos como implementarlo, decidimos obtener los datos de las manos directamente nosotros mismos con una aplicaci√≥n en Python, el archivo `src/extract_hands.py`.
Su funcionamiento redica en que al ejecutarse, pedir√° por la terminal que palabras vas a capturar, creando las carpetas correspondientes, cada vez que detecte una mano capturar√°. Pero, para pasar de toma habr√° que pulsar la tecla `Q` del teclado. Si quieres cerrar el programa, solamente tendr√°s que pulsar `Esc`.

<img src = 'https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/images/Muestradecapturademanos.png' width = 800px>

#### Extracci√≥n de Keypoints de los samples y almacenamiento en archivos HDF
Tras sacar todas las tomas que necesites de esa Palabra, se ejecutar√° `process_hands.py` para procesar los keypoints de cada frame almacenado en cada sample, nos generar√° un archivo `<palabra>.h5`, donde se almacenar√° el Dataframe de la palabra.

<img src = 'https://github.com/GuillermoRojoSantos/GestoLingo/blob/main/images/muestradesamplesydataframes.png' width = 800px>

# Limpieza de datos

# Exploraci√≥n y visualizaci√≥n de los datos

# Preparaci√≥n de los datos

# Entrenamiento del modelo y comprobaci√≥n del rendimiento

# Procesamiento de Lenguaje Natural

# Aplicaci√≥n Web

# Bibliograf√≠a

# Conclusi√≥n